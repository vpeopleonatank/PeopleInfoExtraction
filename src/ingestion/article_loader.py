"""CLI to fetch Thanh Niên articles and emit cleaned passages."""
from __future__ import annotations

import argparse
import json
import logging
from pathlib import Path
from typing import Iterable, List, Optional

try:
    from tqdm import tqdm
except ImportError:  # pragma: no cover - fallback when tqdm missing
    tqdm = lambda x, **kwargs: x  # type: ignore

from src import config
from src.ingestion.articles import parse_article_html, write_article_json
from src.utils.http import fetch_text

LOGGER = logging.getLogger(__name__)


def load_timeline(path: Path) -> List[dict]:
    with path.open("r", encoding="utf-8") as fh:
        return json.load(fh)


def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fetch and clean Thanh Niên Pháp Luật articles")
    parser.add_argument(
        "--timeline-json",
        type=Path,
        default=config.CACHE_DIR / "phap_luat_articles.json",
        help="Path to JSON file generated by the timeline scraper",
    )
    parser.add_argument("--limit", type=int, default=20, help="Maximum number of articles to process")
    parser.add_argument("--skip-existing", action="store_true", help="Skip articles whose JSON exists")
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=config.CLEAN_TEXT_DIR,
        help="Directory to store cleaned article JSON files",
    )
    parser.add_argument("--min-passage-tokens", type=int, default=config.ExtractionConfig().min_passage_tokens)
    parser.add_argument("--max-passage-tokens", type=int, default=config.ExtractionConfig().max_passage_tokens)
    return parser.parse_args(argv)


def main(argv: Optional[Iterable[str]] = None) -> int:
    args = parse_args(argv)
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")
    config.ensure_data_dirs()

    timeline_entries = load_timeline(args.timeline_json)
    if not timeline_entries:
        LOGGER.error("Timeline file %s is empty", args.timeline_json)
        return 1

    processed = 0
    failures: List[str] = []

    for entry in tqdm(timeline_entries[: args.limit], desc="Articles"):
        url = entry.get("canonical_url") or entry.get("url")
        if not url:
            continue
        try:
            html = fetch_text(url)
            document = parse_article_html(html, url, doc_id=entry.get("data_id"))
            output_path = args.output_dir / f"{document.doc_id}.json"
            if args.skip_existing and output_path.exists():
                LOGGER.info("Skipping %s (exists)", document.doc_id)
                processed += 1
                continue
            document.build_passages(
                min_tokens=args.min_passage_tokens,
                max_tokens=args.max_passage_tokens,
            )
            write_article_json(document, output_path)
            raw_path = config.RAW_HTML_DIR / f"{document.doc_id}.html"
            raw_path.parent.mkdir(parents=True, exist_ok=True)
            raw_path.write_text(html, encoding="utf-8")
            processed += 1
        except Exception as exc:  # pragma: no cover - best-effort logging
            LOGGER.exception("Failed to process %s: %s", url, exc)
            failures.append(url)

    LOGGER.info("Processed %s articles (failures: %s)", processed, len(failures))
    if failures:
        LOGGER.warning("Failed URLs: %s", failures)
        return 2
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
